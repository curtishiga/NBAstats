import pandas as pd
import numpy as np
from datetime import datetime
#import pytz
import scipy
import requests
import warnings
import json
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import seaborn as sns

warnings.filterwarnings('ignore')
pd.set_option('display.max_columns',None)

import extract_nba_rolling_stats





stat_categories = ['fpts']


def split_train_test(data, model_method):

    unique_game_dates = data[data['game_date'] >= '2022-01-01']['game_date'].unique()

    # Take 2023 and beyond to adjust for the rolling statistic
    split_date = unique_game_dates[int(len(unique_game_dates) * 0.7)]

    data['fpts_avg_met'] = (data['fpts'] >= data['player_fpts_mean']).astype(int)
    
    train = data[data['game_date'] <= split_date]
    test = data[data['game_date'] > split_date]
        
        # Independent and Dependent variables
    ## Complete
    # ((col == 'min')
    #   | (('opponent_team_opp_' in col)
    #      & ('_stand' in col))
    #  )
    # ((col == 'min')
    #   | (('opponent_team_opp_' in col)
    #      & ('_pg' in col))
    #  )
    ## Try
    # ((('player_' in col)
    #    & ('mean' in col)
    #    & ('league' not in col))
    #   | (('opponent_team_opp_' in col)
    #      & ('_pg' in col))
    #  )
    X_cols = [col for col in data.columns if ((('_mean_stand' in col)
                                               & ('league' not in col))
                                              | (col == 'min_stand')
                                              # | (('team_' in col)
                                              #    & ('_pg_stand' in col)
                                              #    & ('opp_' not in col))
                                              | (('opponent_team_opp_' in col)
                                                 & ('_pg' in col)
                                                & ('_stand' in col))
                                             )
             ]

    if model_method == 'regressor':
        y_cols = [col for col in data.columns if '_normed' in col]
    elif model_method == 'classification':
        y_cols = [col for col in data.columns if '_avg_met' in col]
    
    # Training set
    X_train = train[X_cols].dropna()
    y_train = train.loc[X_train.index,
                        y_cols]
    
    # Test set
    X_test = test[X_cols].dropna()
    y_test = test.loc[X_test.index,
                    y_cols]
    
    return X_train, X_test, y_train, y_test





from sklearn.metrics import mean_squared_error, r2_score





import json
from keras.models import model_from_json

#50, 50, 50, 50, 50, 20, 20, 10, 10, 5
best_model_regress = model_from_json(open('./Models/nn_regress_fpts_(50,50,50,50,50,20,20,10,10,5)_15 rp.json').read())

best_model_regress.load_weights('./Models/nn_regress_fpts_(50,50,50,50,50,20,20,10,10,5)_15 rp.h5')





# ANN regressor rolling period
script_run_extract_15 = extract_nba_rolling_stats.run_extract(15,
                                                          stat_categories)

extract_data_15 = script_run_extract_15.extracted_data
rolling_data_15 = script_run_extract_15.rolling_shifted_data
standard_data_15 = script_run_extract_15.standardized_data


X_train_15, X_test_15, y_train_reg_15, y_test_reg_15 = split_train_test(standard_data_15, model_method = 'regressor')








y_pred_regress_nn = pd.DataFrame(best_model_regress.predict(X_test_15),
                      columns = ['fpts_normed'])


y_test_reg_15['fpts'] = y_test_reg_15['fpts_normed'].apply(lambda x: x**2)
y_pred_regress_nn['fpts'] = y_pred_regress_nn['fpts_normed'].apply(lambda x: x**2)





mse_normed_nn = mean_squared_error(y_test_reg_15[['fpts_normed']],
                         y_pred_regress_nn[['fpts_normed']],
                        squared = True)

rmse_nn = mean_squared_error(y_test_reg_15[['fpts']],
                         y_pred_regress_nn[['fpts']],
                        squared = False)

r_squared_nn = r2_score(y_test_reg_15[['fpts']],
                     y_pred_regress_nn[['fpts']])

print('Normed MSE = %f' %mse_normed_nn)
print('Root Mean Squared Error = %f' %rmse_nn)
print('R Squared = %f' %r_squared_nn)

# Residual plot
residuals_nn = y_test_reg_15['fpts'].values - y_pred_regress_nn['fpts'].values
print('Sum of residuals = %f' %sum(residuals_nn))


fig, axes = plt.subplots(1,2,
                         figsize = (20,10))
sns.residplot(ax = axes[0],
              x = y_test_reg_15['fpts'],
             y = y_pred_regress_nn['fpts'])

sns.histplot(ax = axes[1],
            data = residuals_nn,
           kde = True)
plt.show()

print(scipy.stats.skew(residuals_nn))











# Save trained model
import pickle

model_file_name = './Models/rf_regress_model_fpts.pkl'

with open(model_file_name, 'rb') as file:
    final_rf_regress = pickle.load(file)





# RandomForest regressor rolling period
script_run_extract_30 = extract_nba_rolling_stats.run_extract(30,
                                                          stat_categories)

extract_data_30 = script_run_extract_30.extracted_data
rolling_data_30 = script_run_extract_30.rolling_shifted_data
standard_data_30 = script_run_extract_30.standardized_data


X_train_30, X_test_30, y_train_reg_30, y_test_reg_30 = split_train_test(standard_data_30, model_method = 'regressor')





y_pred_regress_rf = pd.DataFrame(final_rf_regress.predict(X_test_30),
                      columns = ['fpts_normed']
                                )


y_test_reg_30['fpts'] = y_test_reg_30['fpts_normed'].apply(lambda x: x**2)
y_pred_regress_rf['fpts'] = y_pred_regress_rf['fpts_normed'].apply(lambda x: x**2)





mse = mean_squared_error(y_test_reg_30[['fpts']],
                         y_pred_regress_rf[['fpts']],
                        squared = False)

r_squared = r2_score(y_test_reg_30[['fpts']],
                     y_pred_regress_rf[['fpts']])

print('Root Mean Squared Error = %f' %mse)
print('R Squared = %f' %r_squared)

# Residual plot
residuals_rf = y_test_reg_30['fpts'].values - y_pred_regress_rf['fpts'].values
print('Sum of residuals = %f' %sum(residuals_rf))





fig, axes = plt.subplots(1,2,
                         figsize = (20,10))
sns.residplot(ax = axes[0],
              x = y_test_reg_30['fpts'],
             y = y_pred_regress_rf['fpts'])

sns.histplot(ax = axes[1],
            data = residuals_nn,
           kde = True)
plt.show()

print(scipy.stats.skew(residuals_nn))








from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay, roc_auc_score, classification_report, roc_curve





best_model = model_from_json(open('./Models/nn_class_fpts_(50,50,20,10,5,5)_50 rp.json').read())

best_model.load_weights('./Models/nn_class_fpts_(50,50,20,10,5,5)_50 rp.h5')





# ANN classifier rolling period
script_run_extract_50 = extract_nba_rolling_stats.run_extract(50,
                                                          stat_categories)

extract_data_50 = script_run_extract_50.extracted_data
rolling_data_50 = script_run_extract_50.rolling_shifted_data
standard_data_50 = script_run_extract_50.standardized_data


X_train_50, X_test_50, y_train_class_50, y_test_class_50 = split_train_test(standard_data_50, model_method = 'classification')





y_pred_class_nn = pd.DataFrame(best_model.predict(X_test_50),
                      columns = ['fpts_avg_met'])





y_pred_proba_class_nn = best_model.predict(X_test_50)


y_pred_class_nn = y_pred_proba_class_nn.copy()

threshold_nn = 0.5

y_pred_class_nn[y_pred_class_nn >= threshold_nn] = 1
y_pred_class_nn[y_pred_class_nn < threshold_nn] = 0


nn_class_cm = confusion_matrix(y_test_class_50,
                                 y_pred_class_nn)


cm_plot = ConfusionMatrixDisplay(nn_class_cm)
cm_plot.plot()
plt.show()


tp_nn,fp_nn,fn_nn,tn_nn = nn_class_cm.ravel()


sensitivity_nn = tp_nn/(tp_nn+fn_nn)
specificity_nn = tn_nn/(tn_nn+fp_nn)
precision_nn = tp_nn/(tp_nn+fp_nn)





print(classification_report(y_test_class_50, y_pred_class_nn))
print('ROCAUC = %.3f' %roc_auc_score(y_test_class_50, y_pred_proba_class_nn))
print('Sensitivity = %.3f' %sensitivity_nn)
print('Sepcificity = %.3f' %specificity_nn)
print('Precision = %.3f' %precision_nn)








# Save trained model
import pickle

model_file_name = './Models/rf_class_model_fpts.pkl'

with open(model_file_name, 'rb') as file:
    final_rf_class = pickle.load(file)





# RandomForest classifier rolling period
script_run_extract_60 = extract_nba_rolling_stats.run_extract(60,
                                                          stat_categories)

extract_data_60 = script_run_extract_60.extracted_data
rolling_data_60 = script_run_extract_60.rolling_shifted_data
standard_data_60 = script_run_extract_60.standardized_data


X_train_60, X_test_60, y_train_class_60, y_test_class_60 = split_train_test(standard_data_60, model_method = 'classification')





y_pred_class_rf = pd.DataFrame(final_rf_class.predict(X_test_60),
                      columns = ['fpts_avg_met'])





y_pred_class_rf = final_rf_class.predict(X_test_60)
y_pred_proba_rf = final_rf_class.predict_proba(X_test_60)


rf_class_cm = confusion_matrix(y_test_class_60,
                                 y_pred_class_rf)


cm_plot = ConfusionMatrixDisplay(rf_class_cm)
cm_plot.plot()
plt.show()


tp_rf,fp_rf,fn_rf,tn_rf = rf_class_cm.ravel()


sensitivity_rf = tp_rf/(tp_rf+fn_rf)
specificity_rf = tn_rf/(tn_rf+fp_rf)
precision_rf = tp_rf/(tp_rf+fp_rf)


y_pred_proba_rf[1]


print(classification_report(y_test_class_60, y_pred_class_rf))
print('ROCAUC = %.3f' %roc_auc_score(y_test_class_60, y_pred_proba_rf[:,1]))
print('Sensitivity = %.3f' %sensitivity_rf)
print('Sepcificity = %.3f' %specificity_rf)
print('Precision = %.3f' %precision_rf)














actuals = y_test_reg_30[['fpts']].rename(columns = {'fpts':'fpts_actual'})


nn_regress_pred = y_pred_regress_nn[['fpts']].rename(columns = {'fpts':'fpts_nn_regress'})
nn_regress_pred.index = actuals.index


rf_regress_pred = y_pred_regress_rf[['fpts']].rename(columns = {'fpts':'fpts_rf_regress'})
rf_regress_pred.index = actuals.index


nn_class_pred = pd.DataFrame(y_pred_class_nn,
                             columns = ['fpts_nn_class'])
nn_class_pred.index = actuals.index


rf_class_pred = pd.DataFrame(y_pred_class_rf,
                             columns = ['fpts_rf_class'])
rf_class_pred.index = actuals.index





all_y = pd.concat([actuals,
                   nn_regress_pred,
                   rf_regress_pred,
                   nn_class_pred,
                   rf_class_pred],
                  axis = 1,
                 ignore_index = False)





X_data_meta = standard_data_15[standard_data_15.index.isin(all_y.index)]\
                [['game_date','player_id','player_position',
                 'team_id','opponent_team_id',
                 'min','player_fpts_per36']]


X_data_15 = standard_data_15[standard_data_15.index.isin(all_y.index)]\
                [['player_fpts_mean',
                 # 'opponent_team_opp_fpts_pg'
                 ]]\
                .rename(columns = {'player_fpts_mean':'player_fpts_mean_15',
                                   # 'opponent_team_opp_fpts_pg':'opponent_team_opp_fpts_pg_15'
                                  })


X_data_30 = standard_data_30[standard_data_30.index.isin(all_y.index)]\
                [['player_fpts_mean',
                 # 'opponent_team_opp_fpts_pg'
                 ]]\
                .rename(columns = {'player_fpts_mean':'player_fpts_mean_30',
                                   # 'opponent_team_opp_fpts_pg':'opponent_team_opp_fpts_pg_30'
                                  })


X_data_50 = standard_data_50[standard_data_50.index.isin(all_y.index)]\
                [['player_fpts_mean',
                 # 'opponent_team_opp_fpts_pg'
                 ]]\
                .rename(columns = {'player_fpts_mean':'player_fpts_mean_50',
                                   # 'opponent_team_opp_fpts_pg':'opponent_team_opp_fpts_pg_50'
                                  })


X_data_60 = standard_data_60[standard_data_60.index.isin(all_y.index)]\
                [['player_fpts_mean',
                 # 'opponent_team_opp_fpts_pg'
                 ]]\
                .rename(columns = {'player_fpts_mean':'player_fpts_mean_60',
                                   # 'opponent_team_opp_fpts_pg':'opponent_team_opp_fpts_pg_60'
                                  })





all_X = pd.concat([X_data_meta,
                   X_data_15,
                   X_data_30,
                   X_data_50,
                   X_data_60],
                  axis = 1,
                  ignore_index = False)








eval_test_data = pd.concat([all_X,
                       all_y],
                      axis = 1)


# Did regression models predict average was met?
eval_test_data['fpts_nn_regress_class'] = (eval_test_data['fpts_nn_regress'] >= eval_test_data['player_fpts_mean_15']).astype(int)
eval_test_data['fpts_rf_regress_class'] = (eval_test_data['fpts_rf_regress'] >= eval_test_data['player_fpts_mean_30']).astype(int)

# True values if player met averages
for i in [15, 30, 50, 60]:
    eval_test_data['fpts_avg_met_%i' %i] = (eval_test_data['fpts_actual'] >= eval_test_data['player_fpts_mean_%i' %i]).astype(int)








rf_class_cm = confusion_matrix(y_test_class_60,
                                 y_pred_class_rf)


cm_plot = ConfusionMatrixDisplay(rf_class_cm)
cm_plot.plot()
plt.show()


tp_rf,fp_rf,fn_rf,tn_rf = rf_class_cm.ravel()


sensitivity_rf = tp_rf/(tp_rf+fn_rf)
specificity_rf = tn_rf/(tn_rf+fp_rf)
precision_rf = tp_rf/(tp_rf+fp_rf)
accuracy_rf = (tp_rf + tn_rf)/(rf_class_cm.sum())


y_pred_proba_rf[1]


print(classification_report(y_test_class_60, y_pred_class_rf))
print('ROCAUC = %.3f' %roc_auc_score(y_test_class_60, y_pred_proba_rf[:,1]))
print('Sensitivity = %.3f' %sensitivity_rf)
print('Sepcificity = %.3f' %specificity_rf)
print('Precision = %.3f' %precision_rf)


rp_cols = {15:['fpts_nn_regress_class','fpts_avg_met_15','NN Regression Class'],
           30:['fpts_rf_regress_class','fpts_avg_met_30','RF Regression Class'],
           50:['fpts_nn_class','fpts_avg_met_50','NN Classification'],
           60:['fpts_rf_class','fpts_avg_met_60','RF Classification']
          }

fig,axes = plt.subplots(1,4,
                      figsize = (10,40))
metrics_df = pd.DataFrame()

for i in rp_cols:
    class_cm = confusion_matrix(eval_test_data[rp_cols[i][1]],
                                 eval_test_data[rp_cols[i][0]])

    cm_plot = ConfusionMatrixDisplay(class_cm)
    cm_plot.plot(ax = axes[list(rp_cols.keys()).index(i)],
                colorbar = False)
    cm_plot.ax_.set_title(rp_cols[i][2])

    tp,fp,fn,tn = class_cm.ravel()
    
    sensitivity = tp/(tp+fn)
    specificity = tn/(tn+fp)
    precision = tp/(tp+fp)
    accuracy = (tp + tn)/(class_cm.sum())

    model_metrics_df = pd.DataFrame([sensitivity,
                                     specificity,
                                     precision,
                                     accuracy],
                                    index = ['Sensitivity',
                                             'Specificity',
                                             'Precision',
                                             'Accuracy'],
                                   columns = [rp_cols[i][2]])
    metrics_df = pd.concat([metrics_df,
                           model_metrics_df],
                          axis = 1)

plt.tight_layout()
plt.show()


metrics_df





# NN Classification performs best based on accuracy
# Perfer accuracy as metric due to desire to predict if player meet or don't meet their average


















